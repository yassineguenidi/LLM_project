import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# -------------------------------------------------------------------------
# 1. Load model & tokenizer
# -------------------------------------------------------------------------
MODEL_PATH = "./best_model"

@st.cache_resource
def load_model():

    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    return tokenizer, model

tokenizer, model = load_model()
st.title("ğŸ¤– LLM Judge â€“ Compare Two AI Responses")
st.write("Cette interface utilise ton modÃ¨le finetunÃ© pour choisir la meilleure rÃ©ponse entre A et B.")
st.write("                                                ")




# -------------------------------------------------------------------------
# 2. User Input
# -------------------------------------------------------------------------
st.subheader("ğŸ“ Prompt")
prompt = st.text_area("Entre le prompt :", height=120)

st.subheader("ğŸ’¬ RÃ©ponse A")
response_a = st.text_area("Texte de la rÃ©ponse A :", height=150)

st.subheader("ğŸ’¬ RÃ©ponse B")
response_b = st.text_area("Texte de la rÃ©ponse B :", height=150)

# -------------------------------------------------------------------------
# 3. Predict button
# -------------------------------------------------------------------------
if st.button("ğŸ” Comparer les rÃ©ponses"):
    if not prompt or not response_a or not response_b:
        st.error("Veuillez remplir tous les champs.")

        with st.spinner("Analyse en cours..."):

            # -------------------------------------------------------------
            # Build the input text exactly as during training
            # -------------------------------------------------------------
            input_text = (
                f"Prompt:\n{prompt}\n\n"
                f"RÃ©ponse A:\n{response_a}\n\n"
                f"RÃ©ponse B:\n{response_b}\n\n"
                "Laquelle est meilleure ?"
            )

            # Tokenize
            inputs = tokenizer(
                input_text,
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt"
            )

            # Model prediction
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1).flatten().tolist()

            prob_A = probs[0]
            prob_B = probs[1]

            # -------------------------------------------------------------
            # Display results
            # -------------------------------------------------------------
            st.subheader("ğŸ“Š RÃ©sultats")

            st.write(f"**ProbabilitÃ© que la meilleure rÃ©ponse soit A :** `{prob_A:.3f}`")
            st.write(f"**ProbabilitÃ© que la meilleure rÃ©ponse soit B :** `{prob_B:.3f}`")

            if prob_A > prob_B:
                st.success("ğŸŸ¢ **RÃ©ponse A gagnante !**")
            elif prob_B > prob_A:
                st.success("ğŸ”µ **RÃ©ponse B gagnante !**")
            else:
                st.warning("âšª EgalitÃ© parfaite entre A et B.")


# Footer
st.markdown("---")
st.write("CrÃ©Ã© par Yassine â€“ ModÃ¨le finetunÃ© LLM Judge")
